# -*- coding: utf-8 -*-
"""NST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfVHLkppf6w6E9mrnxO_bF8wd9jinO_M
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Image size (keep small for speed)
IMAGE_SIZE = 320

loader = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor()
])

def load_image(image_path):
    image = Image.open(image_path).convert("RGB")
    image = loader(image).unsqueeze(0)  # (1, 3, H, W)
    return image.to(device)

# load image

content_img = load_image("content.jpg")
style_img = load_image("style.jpg")

print(content_img.shape, style_img.shape)

# visualize image

def imshow(tensor, title=None):
    image = tensor.clone().detach().cpu().squeeze(0)
    image = image.permute(1, 2, 0)
    plt.imshow(image)
    if title:
        plt.title(title)
    plt.axis("off")
    plt.show()

imshow(content_img, "Content Image")
imshow(style_img, "Style Image")

class CNN(nn.Module):
    def __init__(self):
        super().__init__()

        def block(in_c, out_c):
            return nn.Sequential(
                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),
                nn.InstanceNorm2d(out_c),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),
                nn.InstanceNorm2d(out_c),
                nn.ReLU(inplace=True),
            )

        self.b1 = block(3, 32)
        self.p1 = nn.MaxPool2d(2)

        self.b2 = block(32, 64)
        self.p2 = nn.MaxPool2d(2)

        self.b3 = block(64, 128)
        self.p3 = nn.MaxPool2d(2)

        self.b4 = block(128, 256)

    def forward(self, x):
        features = []

        x = self.b1(x)
        features.append(x)
        x = self.p1(x)

        x = self.b2(x)
        features.append(x)
        x = self.p2(x)

        x = self.b3(x)
        features.append(x)
        x = self.p3(x)

        x = self.b4(x)
        features.append(x)

        return features

# model initialze

model = CNN().to(device)

# extract feature maps

content_features = model(content_img)
style_features = model(style_img)

# inspect feature shapes

for i, feat in enumerate(content_features):
    print(f"Layer {i} feature shape: {feat.shape}")

# visualize feature maps

def show_feature_maps(feature_tensor, num_maps=6):
    feature_tensor = feature_tensor[0]  # remove batch
    plt.figure(figsize=(15, 5))

    for i in range(num_maps):
        plt.subplot(1, num_maps, i + 1)
        plt.imshow(feature_tensor[i].detach().cpu(), cmap="gray")
        plt.axis("off")

    plt.show()

show_feature_maps(content_features[0])   # shallow
show_feature_maps(content_features[-1])  # deep

# freeze CNN

for param in model.parameters():
    param.requires_grad = False

# Initialize the Generated Image (CORRECT)
generated_img = (
    0.5 * content_img +
    0.5 * torch.rand_like(content_img)
).clamp(0, 1)

generated_img.requires_grad_(True)

print(
    generated_img.min().item(),
    generated_img.max().item()
)

# Choose Content & Style Layers
CONTENT_LAYER = 3
STYLE_LAYERS = [1, 2, 3]

# Loss weights
alpha = 5        # content
beta = 2000         # style (dominant)
tv_weight = 2e-4   # very small smoothing

# Optimization
num_steps = 1200
lr = 0.01

# Content Loss

def content_loss(gen_feat, content_feat):
    return torch.mean((gen_feat - content_feat) ** 2)

# Gram Matrix
def gram_matrix(x):
    b, c, h, w = x.size()
    features = x.view(c, h * w) - x.mean(dim=(2,3)).view(c,1)
    gram = torch.mm(features, features.t())
    return gram / (c * h * w)

# Style Loss

def style_loss(gen_features, style_features):
    loss = 0
    for i in STYLE_LAYERS:
        g = gram_matrix(gen_features[i])
        s = gram_matrix(style_features[i])
        loss += torch.mean((g - s) ** 2)
    return loss / len(STYLE_LAYERS)

def total_loss(gen_features, content_features, style_features, img, step):
    # ðŸ”» Reduce content influence over time
    content_weight = max(0.1, alpha * (1 - step / num_steps))

    c_loss = content_loss(
        gen_features[CONTENT_LAYER],
        content_features[CONTENT_LAYER]
    )
    s_loss = style_loss(gen_features, style_features)
    tv_loss = total_variation_loss(img)

    return (
        content_weight * c_loss +
        beta * s_loss +
        tv_weight * tv_loss
    )

def total_variation_loss(img):
    return (
        torch.mean(torch.abs(img[:, :, :-1] - img[:, :, 1:])) +
        torch.mean(torch.abs(img[:, :-1, :] - img[:, 1:, :]))
    )

# Extract content & style features ONCE (no gradients)
with torch.no_grad():
    content_features = model(content_img)
    style_features = model(style_img)

show_feature_maps(style_features[-1])

optimizer = optim.Adam([generated_img], lr)

for step in range(num_steps):
    optimizer.zero_grad()

    # ðŸ”¹ Forward pass (DO NOT use torch.no_grad here)
    gen_features = model(generated_img)

    # ðŸ”¹ Compute loss
    style_scale = min(1.0, step / 300)

    if step < 300:
      content_scale = 1.0
    elif step < 800:
      content_scale = 0.3
    else:
      content_scale = 0.1

    loss = (
        content_scale * content_loss(
        gen_features[CONTENT_LAYER],
        content_features[CONTENT_LAYER]
      )
      + beta * style_loss(gen_features, style_features)
      + tv_weight * total_variation_loss(generated_img)
    )

    # ðŸ”¹ Backward pass
    loss.backward()

    # ðŸ”¹ DEBUG 1: Check gradients (ONLY at step 0)
    if step == 0:
        print("Gradient mean:", generated_img.grad.abs().mean().item())

    # ðŸ”¹ Optimizer step
    optimizer.step()

    # ðŸ”¹ Clamp image values
    with torch.no_grad():
        if step % 25 == 0:
          generated_img.clamp_(0, 1)

    # ðŸ”¹ DEBUG 2: Check image is changing
    if step % 50 == 0:
        diff = torch.mean(
            torch.abs(generated_img - content_img)
        ).item()
        print(
            f"Step {step}, Loss: {loss.item():.4f}, "
            f"mean pixel diff from content: {diff:.4f}"
        )

# save image

def save_image(tensor, filename):
    image = tensor.clone().detach().cpu().squeeze(0)
    image = image.permute(1, 2, 0)

    # ðŸ”‘ CRITICAL FIX
    image = image - image.min()
    image = image / image.max()

    plt.imsave(filename, image)

save_image(generated_img, "stylized_output.png")

# comparision

plt.figure(figsize=(12,4))

plt.subplot(1,3,1)
imshow(content_img, "Content")

plt.subplot(1,3,2)
imshow(style_img, "Style")

plt.subplot(1,3,3)
imshow(generated_img, "Generated")