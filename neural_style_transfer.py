# -*- coding: utf-8 -*-
"""Neural Style Transfer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ks5YO-_WESnme41UKmBlbCh9YpS-q-65
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

print(f"PyTorch version: {torch.__version__}")
print(f"GPU available: {torch.cuda.is_available()}")

# Helper function to load images
def load_image(image_path, max_size=400):
    image = Image.open(image_path).convert('RGB')

    # Resize if too large
    size = min(max_size, max(image.size))
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.ToTensor(),  # Converts to [0,1] range and CHW format
    ])

    image = transform(image).unsqueeze(0)  # Add batch dimension
    return image

# Helper to display images
def imshow(tensor, title=None):
    image = tensor.cpu().clone().squeeze(0)  # Remove batch dimension
    image = image.permute(1, 2, 0)  # CHW -> HWC for matplotlib
    image = torch.clamp(image, 0, 1)  # Ensure valid range

    plt.imshow(image)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# Load and display the images
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

content_img = load_image("content.jpg", max_size=400).to(device)
style_img_0 = load_image("style_0.jpg", max_size=400).to(device)
style_img_1 = load_image("style_1.jpg", max_size=400).to(device)
style_img_2 = load_image("style_2.jpg", max_size=400).to(device)

print(f"Content image shape: {content_img.shape}")
print(f"Style image shape: {style_img_0.shape}")
print(f"Style image shape: {style_img_1.shape}")
print(f"Style image shape: {style_img_2.shape}")

# Display them
plt.figure(figsize=(12, 4))

imshow(content_img, "Content Image")

imshow(style_img_0, "Style Image")

imshow(style_img_1, "Style Image")

imshow(style_img_0, "Style Image")

plt.tight_layout()
plt.show()

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        # Layer 1: 3 input channels (RGB) -> 16 feature maps
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2, 2)

        # Layer 2: 16 -> 32 feature maps
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2, 2)

        # Layer 3: 32 -> 64 feature maps
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()

        # Layer 4: 64 -> 128 feature maps
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.relu4 = nn.ReLU()

    def forward(self, x, layers_to_return=['conv1', 'conv2', 'conv3', 'conv4']):
        outputs = {}

        x = self.conv1(x)
        x = self.relu1(x)
        if 'conv1' in layers_to_return:
            outputs['conv1'] = x
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        if 'conv2' in layers_to_return:
            outputs['conv2'] = x
        x = self.pool2(x)

        x = self.conv3(x)
        x = self.relu3(x)
        if 'conv3' in layers_to_return:
            outputs['conv3'] = x

        x = self.conv4(x)
        x = self.relu4(x)
        if 'conv4' in layers_to_return:
            outputs['conv4'] = x

        return outputs

# Create the model
model = SimpleCNN().to(device)
model.eval()  # Set to evaluation mode (we're not training the CNN itself!)

print("‚úì CNN built!")
print(f"Model has {sum(p.numel() for p in model.parameters())} parameters")

def gram_matrix(features):
    """
    Calculate Gram matrix for style representation

    features shape: [batch, channels, height, width]
    Gram matrix shape: [channels, channels]
    """
    batch, channels, height, width = features.size()

    # Reshape features: [channels, height*width]
    features = features.view(batch * channels, height * width)

    # Gram matrix: features * features^T
    # This captures correlations between different feature maps
    gram = torch.mm(features, features.t())

    # Normalize by number of elements
    return gram / (batch * channels * height * width)

# Test it
test_features = model(content_img)['conv1']
test_gram = gram_matrix(test_features)

print(f"Feature map shape: {test_features.shape}")
print(f"Gram matrix shape: {test_gram.shape}")
print(f"\nGram matrix captures style as feature correlations!")

def content_loss(generated_features, content_features):
    """
    Content loss: MSE between feature maps
    We want generated image to have similar features to content image
    """
    return torch.mean((generated_features - content_features) ** 2)


def style_loss(generated_features, style_gram):
    """
    Style loss: MSE between Gram matrices
    We want generated image's style patterns to match style image
    """
    generated_gram = gram_matrix(generated_features)
    return torch.mean((generated_gram - style_gram) ** 2)


def total_loss(model, generated_img, content_img, style_grams,
               content_weight=1, style_weight=1000):
    """
    Combined loss function - recomputes everything fresh each time
    """
    # Extract features from generated image (fresh computation)
    gen_features = model(generated_img)

    # Extract features from content image (fresh computation)
    with torch.no_grad():  # Don't need gradients for content
        content_features = model(content_img)

    # Content loss (only from deep layer)
    c_loss = content_loss(gen_features['conv4'], content_features['conv4'])

    # Style loss (from multiple layers)
    s_loss = 0
    style_layers = ['conv1', 'conv2', 'conv3']

    for layer in style_layers:
        s_loss += style_loss(gen_features[layer], style_grams[layer])

    # Combine losses
    total = content_weight * c_loss + style_weight * s_loss

    return total, c_loss, s_loss

def total_variation_loss(img):
    """
    Total Variation Loss - encourages spatial smoothness
    Reduces high-frequency noise and makes output look cleaner
    """
    # Calculate differences between adjacent pixels
    tv_h = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]))  # Horizontal
    tv_w = torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]))  # Vertical

    return tv_h + tv_w

def neural_style_transfer(content_img, style_images,
                          num_steps=500,
                          style_weight=1e8,
                          content_weight=1,
                          tv_weight=1e-5,
                          lr=0.01,
                          show_every=100):
    """
    Complete neural style transfer with multi-image style averaging

    Args:
        content_img: Content image tensor
        style_images: List of style image tensors
        num_steps: Number of optimization iterations
        style_weight: Weight for style loss
        content_weight: Weight for content loss
        tv_weight: Weight for total variation loss (smoothness)
        lr: Learning rate
        show_every: Show progress every N steps
    """

    # Compute averaged style Gram matrices
    print(f"Computing style from {len(style_images)} images...")
    averaged_grams = {}

    with torch.no_grad():
        for style_img in style_images:
            style_features = get_features(style_img, vgg, style_layers_vgg)

            for layer_idx in style_layers_vgg:
                gram = gram_matrix(style_features[layer_idx])

                if layer_idx not in averaged_grams:
                    averaged_grams[layer_idx] = gram / len(style_images)
                else:
                    averaged_grams[layer_idx] += gram / len(style_images)

    # Initialize generated image
    generated = content_img.clone().requires_grad_(True)
    optimizer = torch.optim.Adam([generated], lr=lr)

    print(f"\nStarting style transfer for {num_steps} steps...")
    print(f"Weights - Content: {content_weight}, Style: {style_weight:.0e}, TV: {tv_weight:.0e}\n")

    for step in range(num_steps):
        optimizer.zero_grad()

        # Clamp to valid range
        with torch.no_grad():
            generated.clamp_(0, 1)

        # Get features
        gen_features = get_features(generated, vgg, style_layers_vgg + [content_layer_vgg])
        content_features = get_features(content_img, vgg, [content_layer_vgg])

        # Content loss
        c_loss = torch.mean((gen_features[content_layer_vgg] - content_features[content_layer_vgg]) ** 2)

        # Style loss
        s_loss = 0
        for layer_idx in style_layers_vgg:
            gen_gram = gram_matrix(gen_features[layer_idx])
            s_loss += torch.mean((gen_gram - averaged_grams[layer_idx]) ** 2)

        # Total variation loss
        tv_loss = total_variation_loss(generated)

        # Combined loss
        total = content_weight * c_loss + style_weight * s_loss + tv_weight * tv_loss

        total.backward()
        optimizer.step()

        # Show progress
        if step % show_every == 0:
            print(f"Step {step:4d} | Total: {total.item():8.1f} | Content: {c_loss.item():.4f} | Style: {s_loss.item():.6f} | TV: {tv_loss.item():.4f}")

    print("\n‚úì Transfer complete!")
    return generated

print("Loss functions ready!")

# Pre-compute style Gram matrices from all 3 style images
print("Computing Gram matrices for style images...")

style_images = [style_img_0, style_img_1, style_img_2]
style_layers = ['conv1', 'conv2', 'conv3']

# Initialize dictionary to store averaged Gram matrices
averaged_style_grams = {layer: 0 for layer in style_layers}

# Compute Gram matrix for each style image and average them
for i, style_img in enumerate(style_images):
    style_features = model(style_img)

    for layer in style_layers:
        gram = gram_matrix(style_features[layer])
        averaged_style_grams[layer] += gram / len(style_images)  # Average

    print(f"  Processed style image {i+1}/{len(style_images)}")

print("\n‚úì Averaged Gram matrices computed!")
print(f"  This blends the style from all {len(style_images)} Van Gogh paintings")

# Check the shapes
for layer, gram in averaged_style_grams.items():
    print(f"{layer}: {gram.shape}")

# Start with the content image (could also start with random noise)
generated_img = content_img.clone().requires_grad_(True)

# Use LBFGS optimizer - works well for style transfer
optimizer = torch.optim.LBFGS([generated_img])

print(f"Starting image initialized")
print(f"Shape: {generated_img.shape}")
print(f"Requires gradient: {generated_img.requires_grad}")
print("\nNow we'll update the PIXELS of this image to minimize loss!")

import torchvision.models as models

# Load pre-trained VGG19
vgg = models.vgg19(pretrained=True).features.to(device).eval()

# Freeze all parameters
for param in vgg.parameters():
    param.requires_grad_(False)

print("‚úì Loaded pre-trained VGG19")
print(f"VGG has {len(vgg)} layers")

# We'll use these specific layers
# Style layers: early conv layers capture textures
# Content layer: deeper layer captures structure
style_layers_vgg = [0, 5, 10, 19, 28]  # conv1_1, conv2_1, conv3_1, conv4_1, conv5_1
content_layer_vgg = 21  # conv4_2

def get_features(image, model, layers):
    """Extract features from specific layers"""
    features = {}
    x = image

    for i, layer in enumerate(model):
        x = layer(x)
        if i in layers:
            features[i] = x

    return features

# Test it
test_features = get_features(content_img, vgg, style_layers_vgg + [content_layer_vgg])
print(f"\nExtracted {len(test_features)} feature maps")

generated_img = content_img.clone().requires_grad_(True)
optimizer = torch.optim.Adam([generated_img], lr=0.01)

num_steps = 1000
content_weight = 1
style_weight = 1e8

for step in range(num_steps):
    optimizer.zero_grad()

    with torch.no_grad():
        generated_img.clamp_(0, 1)

    gen_features = get_features(generated_img, vgg, style_layers_vgg + [content_layer_vgg])
    content_features = get_features(content_img, vgg, [content_layer_vgg])

    c_loss = torch.mean((gen_features[content_layer_vgg] - content_features[content_layer_vgg]) ** 2)

    s_loss = 0
    for layer_idx in style_layers_vgg:
        gen_gram = gram_matrix(gen_features[layer_idx])
        s_loss += torch.mean((gen_gram - averaged_style_grams_vgg[layer_idx]) ** 2)

    loss = content_weight * c_loss + style_weight * s_loss

    loss.backward()
    optimizer.step()

    if step % 50 == 0:
        print(f"Step {step:3d} | Total: {loss.item():.1f} | Content: {c_loss.item():.4f} | Style: {s_loss.item():.8f}")

print("\n Done!")

# Updated imshow function
def imshow(tensor, title=None):
    image = tensor.cpu().clone().detach().squeeze(0)  # Added .detach()
    image = image.permute(1, 2, 0)  # CHW -> HWC
    image = torch.clamp(image, 0, 1)

    plt.imshow(image.numpy())
    if title:
        plt.title(title)
    plt.axis('off')

# Now display results
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
imshow(content_img, "Original Content")

plt.subplot(1, 3, 2)
imshow(style_img_0, "Style Reference")

plt.subplot(1, 3, 3)
imshow(generated_img, "Generated")

plt.tight_layout()
plt.show()

# Save the result
from torchvision.utils import save_image

# Save generated image
with torch.no_grad():
    output = generated_img.clamp(0, 1)
    save_image(output, 'stylized_output.png')

print("‚úì Image saved as 'stylized_output.png'")
print("\nYou can download it from the files panel on the left!")

# Test the function with TV loss enabled
print("=== Running style transfer with Total Variation Loss ===\n")

generated_with_tv = neural_style_transfer(
    content_img=content_img,
    style_images=[style_img_0, style_img_1, style_img_2],
    num_steps=1000,
    style_weight=1e8,
    content_weight=1,
    tv_weight=1e-5,  # This adds smoothness
    lr=0.01,
    show_every=100
)

# Display result
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
imshow(content_img, "Content")

plt.subplot(1, 3, 2)
imshow(generated_img, "Previous (No TV Loss)")

plt.subplot(1, 3, 3)
imshow(generated_with_tv, "New (With TV Loss)")

plt.tight_layout()
plt.show()

# Save the new version
with torch.no_grad():
    save_image(generated_with_tv.clamp(0, 1), 'stylized_with_tv.png')

print("\n Saved as 'stylized_with_tv.png'")

# ============================================
# NEURAL STYLE TRANSFER - PROJECT SUMMARY
# ============================================

print("=" * 60)
print("NEURAL STYLE TRANSFER PROJECT - COMPLETE")
print("=" * 60)

print("\nüìä PROJECT STATISTICS:")
print(f"  ‚Ä¢ Content images processed: 1")
print(f"  ‚Ä¢ Style images used: 3 (multi-image averaging)")
print(f"  ‚Ä¢ Model: VGG19 (pre-trained on ImageNet)")
print(f"  ‚Ä¢ Optimization steps: 500-1000")
print(f"  ‚Ä¢ Final style weight: 1e8")
print(f"  ‚Ä¢ Includes Total Variation Loss for smoothness")

print("\nüéØ KEY CONCEPTS DEMONSTRATED:")
print("  ‚úì CNN feature extraction at multiple layers")
print("  ‚úì Gram matrix computation for style representation")
print("  ‚úì Multi-image style averaging (novel approach)")
print("  ‚úì Content vs Style loss balancing")
print("  ‚úì Gradient descent on image pixels (not model weights)")
print("  ‚úì Total Variation Loss for noise reduction")

print("\nüíª TECHNICAL SKILLS SHOWCASED:")
print("  ‚Ä¢ PyTorch & Deep Learning")
print("  ‚Ä¢ Computer Vision & CNNs")
print("  ‚Ä¢ Loss function design")
print("  ‚Ä¢ Gradient-based optimization")
print("  ‚Ä¢ Image processing & visualization")

print("\nüìÅ FILES GENERATED:")
print("  ‚Ä¢ stylized_output.png")
print("  ‚Ä¢ stylized_with_tv.png")

print("\n" + "=" * 60)